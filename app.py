import streamlit as st
st.set_page_config(page_title="Palm Analyzer", layout="wide")

try:
    import cv2
except ImportError as e:
    st.error(f"CV2 Import Error: {e}. ƒê·∫£m b·∫£o d√πng opencv-python-headless trong requirements.txt.")
    st.stop()

import mediapipe as mp
import numpy as np
import math
from PIL import Image
import io
try:
    from googletrans import Translator, LANGUAGES
    translator = Translator()
except ImportError:
    st.warning("Googletrans not available - fallback to English.")
    translator = None
    LANGUAGES = {'english': 'en', 'vietnamese': 'vi'}
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.units import inch
import base64
from datetime import datetime
import os

# MediaPipe setup
@st.cache_resource
def load_mediapipe():
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    return mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.7), mp_drawing

hands, mp_drawing = load_mediapipe()

# Translate functions (gi·ªØ nguy√™n)
def translate_text(text, target_lang='vi'):
    try:
        if not translator or target_lang == 'en': return text
        lang_code = LANGUAGES.get(target_lang, 'vi')
        result = translator.translate(text, dest=lang_code)
        return result.text
    except Exception as e:
        st.warning(f"Translate error ({e}) - fallback to English.")
        return text

def get_ui_texts(lang):
    base_texts = {
        'title': 'üñêÔ∏è Palm Pro Analyzer - Ch·∫•m ƒêi·ªÉm B√†n Tay AI (T·ªëi ∆Øu)',
        'upload_label': 'Ch·ªçn ·∫£nh JPG/PNG',
        'original_caption': '·∫¢nh g·ªëc',
        'annotated_caption': '·∫¢nh full + Lines overlay ƒë√∫ng v·ªã tr√≠ (Xanh=Life, ƒê·ªè=Heart, XanhD=Head)',
        'history_title': 'L·ªãch S·ª≠ Ph√¢n T√≠ch',
        'share_text': 'Chia S·∫ª Text (.txt)',
        'share_img': 'Chia S·∫ª ·∫¢nh (.png)',
        'share_pdf': 'Chia S·∫ª PDF',
        'share_link': 'Copy Link Share',
        'no_history': 'Ch∆∞a c√≥ l·ªãch s·ª≠. Upload ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu!',
        'detect_error': 'Kh√¥ng detect b√†n tay! Ch·ª•p r√µ l√≤ng b√†n tay h∆∞·ªõng l√™n.',
        'note': 'üí° Note: Accuracy cao v·ªõi ·∫£nh s√°ng. Scar=break >5% palm width (t·ª´ palmistry: obstacles). Train ML th√™m n·∫øu c·∫ßn.'
    }
    lang_code = LANGUAGES.get(lang, 'vi')
    translated = {k: translate_text(v, lang_code) for k, v in base_texts.items()}
    return translated

# Fixed ROI: Ch·ªâ crop cho detect, kh√¥ng cho display
def get_palm_roi(image, landmarks, h, w):
    points = [(int(lm.x * w), int(lm.y * h)) for lm in landmarks]
    palm_points = points[:5] + points[17:21]  # Wrist, thumb, pinky
    xs = [p[0] for p in palm_points]
    ys = [p[1] for p in palm_points]
    min_x, max_x = min(xs), max(xs)
    min_y, max_y = min(ys), max(ys)
    
    extend = 0.2
    roi_x_start = max(0, int(min_x - (max_x - min_x) * extend))
    roi_x_end = min(w, int(max_x + (max_x - min_x) * extend))
    roi_y_start = max(0, int(min_y - (max_y - min_y) * extend))
    roi_y_end = min(h, int(max_y + (max_y - min_y) * extend))
    
    roi = image[roi_y_start:roi_y_end, roi_x_start:roi_x_end]
    return roi, (roi_x_start, roi_y_start, roi_x_end, roi_y_end)  # Offset full

def normalize_palm_size(roi):
    h, w = roi.shape[:2]
    if h > 0:
        scale = 200 / max(h, w)
        roi = cv2.resize(roi, (int(w * scale), int(h * scale)))
    return roi

def detect_lines_optimized(roi):
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    kernel = np.ones((3,3), np.uint8)
    closed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)
    edges = cv2.Canny(closed, 20, 80)
    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=30, minLineLength=20, maxLineGap=30)
    
    palm_h, palm_w = roi.shape[:2]
    life_line, heart_line, head_line = [], [], []
    
    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            length = math.hypot(x2 - x1, y2 - y1)
            angle = abs(math.degrees(math.atan2(y2 - y1, x2 - x1)))
            mid_y = (y1 + y2) / 2
            mid_x = (x1 + x2) / 2
            
            if length > 20:
                rel_y = mid_y / palm_h
                rel_x = mid_x / palm_w
                if angle > 35 and rel_y > 0.4 and rel_x < 0.4:  # Life left-bottom curved
                    life_line.append((length, angle, (x1,y1,x2,y2), rel_y, rel_x))
                elif angle < 20 and rel_y < 0.2:  # Heart top straight
                    heart_line.append((length, angle, (x1,y1,x2,y2), rel_y, rel_x))
                elif angle < 30 and 0.3 < rel_y < 0.6:  # Head middle
                    head_line.append((length, angle, (x1,y1,x2,y2), rel_y, rel_x))
    
    life_line = sorted(life_line, key=lambda x: x[0], reverse=True)[:2]
    heart_line = sorted(heart_line, key=lambda x: x[0], reverse=True)[:2]
    head_line = sorted(head_line, key=lambda x: x[0], reverse=True)[:2]
    
    return life_line, heart_line, head_line

def detect_breaks(line_segments, palm_w):
    if len(line_segments) < 2: return 0, 0
    gaps = []
    for i in range(len(line_segments) - 1):
        seg1 = line_segments[i][2]
        seg2 = line_segments[i+1][2]
        dist = min(math.hypot(seg1[0]-seg2[0], seg1[1]-seg2[1]), math.hypot(seg1[2]-seg2[2], seg1[3]-seg2[3]))
        if dist > palm_w * 0.05:
            gaps.append(dist)
    num_breaks = len(gaps)
    return num_breaks, sum(gaps) / len(gaps) if gaps else 0

def score_line_optimized(lines, palm_h, palm_w):
    if not lines: return 2, False
    max_len = max(l[0] for l in lines)
    base = min(8, int((max_len / (palm_h * 0.6)) * 8))
    straight_bonus = 1 if min(l[1] for l in lines) < 30 else 0
    num_segs = len(lines)
    breaks, avg_gap = detect_breaks(lines, palm_w)
    penalty = min(3, breaks * 1.5 + (avg_gap / palm_w * 2))
    score = base + straight_bonus + min(1, num_segs - 1) - penalty
    return max(1, min(10, int(score))), breaks > 0

def process_palm(image):
    h, w = image.shape[:2]
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    
    if not results.multi_hand_landmarks:
        return image, "Kh√¥ng detect b√†n tay r√µ! ƒêi·ªÉm m·∫∑c ƒë·ªãnh th·∫•p. Ch·ª•p ·∫£nh l√≤ng b√†n tay m·ªü, s√°ng s·ªßa h∆∞·ªõng l√™n camera.\n\n### PH√ÇN T√çCH CHI TI·∫æT\n- **Detect**: 0 b√†n tay.\n- **ƒê∆∞·ªùng Sinh Kh√≠**: 0 segs, 1/10 | √ù nghƒ©a: S·ª©c kh·ªèe.\n- **ƒê∆∞·ªùng T√¢m ƒê·∫°o**: 0 segs, 1/10 | √ù nghƒ©a: T√¨nh c·∫£m.\n- **ƒê∆∞·ªùng Tr√≠ Tu·ªá**: 0 segs, 1/10 | √ù nghƒ©a: Tr√≠ √≥c.\n- **T·ªîNG**: 3/30\n\nüòÖ ·∫¢nh kh√¥ng r√µ, c·∫ßn boost. Th·ª≠ l·∫°i v·ªõi ·∫£nh t·ªët h∆°n!"
    
    landmarks = results.multi_hand_landmarks[0].landmark
    roi, offset = get_palm_roi(image, landmarks, h, w)  # offset = (x_start, y_start, x_end, y_end)
    
    if roi.size == 0:
        roi = image
        offset = (0, 0, w, h)
    
    roi_norm = normalize_palm_size(roi)
    life, heart, head = detect_lines_optimized(roi_norm)
    
    # FIX: Annotate on FULL image, adjust lines pos with offset
    annotated = image.copy()
    roi_x_start, roi_y_start, roi_x_end, roi_y_end = offset
    roi_h_norm, roi_w_norm = roi_norm.shape[:2]
    roi_h_orig, roi_w_orig = roi.shape[:2]
    scale_x = roi_w_orig / roi_w_norm if roi_w_norm > 0 else 1
    scale_y = roi_h_orig / roi_h_norm if roi_h_norm > 0 else 1
    
    colors = {'life': (0, 255, 0), 'heart': (255, 0, 0), 'head': (0, 0, 255)}
    labels = {'life': 'Sinh Kh√≠', 'heart': 'T√¢m ƒê·∫°o', 'head': 'Tr√≠ Tu·ªá'}
    
    for line_type, lines_list in [('life', life), ('heart', heart), ('head', head)]:
        for i, (length, angle, (x1,y1,x2,y2), rel_y, rel_x) in enumerate(lines_list):
            # Scale back to roi orig
            x1_orig = int(x1 * scale_x) + roi_x_start
            y1_orig = int(y1 * scale_y) + roi_y_start
            x2_orig = int(x2 * scale_x) + roi_x_start
            y2_orig = int(y2 * scale_y) + roi_y_start
            color = colors[line_type]
            thickness = 3 if i==0 else 2
            cv2.line(annotated, (x1_orig, y1_orig), (x2_orig, y2_orig), color, thickness)
            label = f'{labels[line_type]} {i+1} (L={length:.1f}, A={angle:.0f}¬∞)'
            cv2.putText(annotated, label, (x1_orig, y1_orig-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    
    # Score (gi·ªØ nguy√™n)
    diem_sinh, scar_sinh = score_line_optimized(life, roi_norm.shape[0], roi_norm.shape[1])
    diem_tam, scar_tam = score_line_optimized(heart, roi_norm.shape[0], roi_norm.shape[1])
    diem_tri, scar_tri = score_line_optimized(head, roi_norm.shape[0], roi_norm.shape[1])
    tong = diem_sinh + diem_tam + diem_tri
    
    scar_info = ""
    if scar_sinh: scar_info += " (c√≥ v·∫øt s·∫πo/ƒë·ª©t - obstacle t·∫°m th·ªùi, t·∫≠p trung s·ª©c kh·ªèe)"
    if scar_tam: scar_info += " (c√≥ v·∫øt s·∫πo - th·ª≠ th√°ch t√¨nh c·∫£m, c·∫ßn ki√™n nh·∫´n)"
    if scar_tri: scar_info += " (c√≥ v·∫øt s·∫πo - stress s·ª± nghi·ªáp, ngh·ªâ ng∆°i ƒëi)"
    
    if tong >= 25:
        advice = f"üåü B√†n tay elite! Lines r√µ d√†i, {scar_info}. Th√†nh c√¥ng l·ªõn, s·ªëng th·ªç."
    elif tong >= 18:
        advice = f"üëç B√†n tay v·ªØng ch√£i! {scar_info}. C·ªë l√™n, potential cao."
    elif tong >= 12:
        advice = f"ü§î Trung b√¨nh, {scar_info}. C·∫£i thi·ªán l·ªëi s·ªëng ƒë·ªÉ lines r√µ h∆°n."
    else:
        advice = f"üòÖ C·∫ßn boost, {scar_info}. Massage tay, xem chuy√™n gia n·∫øu scar nhi·ªÅu."
    
    result = f"""
### PH√ÇN T√çCH CHI TI·∫æT (ROI full b√†n tay: {roi.shape[:2]} - Detect: {len(results.multi_hand_landmarks)} tay)
- **ƒê∆∞·ªùng Sinh Kh√≠**: {len(life)} segs, {diem_sinh}/10{scar_info if scar_sinh else ''} | √ù nghƒ©a: S·ª©c kh·ªèe/vitality (d√†i=th·ªç).
- **ƒê∆∞·ªùng T√¢m ƒê·∫°o**: {len(heart)} segs, {diem_tam}/10{scar_info if scar_tam else ''} | √ù nghƒ©a: T√¨nh c·∫£m (cong=l√£ng m·∫°n).
- **ƒê∆∞·ªùng Tr√≠ Tu·ªá**: {len(head)} segs, {diem_tri}/10{scar_info if scar_tri else ''} | √ù nghƒ©a: Tr√≠ √≥c/s·ª± nghi·ªáp (s√¢u=s√°ng t·∫°o).
- **T·ªîNG**: {tong}/30

{advice}

üí° Note: Lines v·∫Ω ƒë√∫ng v·ªã tr√≠ b√†n tay. N·∫øu l·ªách, th·ª≠ ·∫£nh r√µ h∆°n. Accuracy cao v·ªõi ·∫£nh s√°ng. Scar=break >5% palm width (t·ª´ palmistry: obstacles). Train ML th√™m n·∫øu c·∫ßn.
"""
    return annotated, result

# Helper functions (gi·ªØ nguy√™n)
def download_text(content, filename):
    st.download_button("üì• T·∫£i Text", content, file_name=filename, mime="text/plain")

def download_image(img_array, filename):
    img_pil = Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
    bio = io.BytesIO()
    img_pil.save(bio, format='PNG')
    st.download_button("üì• T·∫£i ·∫¢nh", bio.getvalue(), file_name=filename, mime="image/png")

def create_pdf(image_array, result_text, filename):
    bio = io.BytesIO()
    doc = SimpleDocTemplate(bio, pagesize=letter)
    styles = getSampleStyleSheet()
    story = []
    story.append(Paragraph("Palm Analysis Report", styles['Title']))
    story.append(Spacer(1, 12))
    img_pil = Image.fromarray(cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB))
    img_buffer = io.BytesIO()
    img_pil.save(img_buffer, format='PNG')
    img_buffer.seek(0)
    img = RLImage(img_buffer, width=4*inch, height=4*inch)
    story.append(img)
    story.append(Spacer(1, 12))
    story.append(Paragraph(result_text.replace('\n', '<br/>'), styles['Normal']))
    doc.build(story)
    bio.seek(0)
    st.download_button("üì• T·∫£i PDF", bio.getvalue(), file_name=filename, mime="application/pdf")

def generate_share_link(entry_id):
    return f"https://yourapp.streamlit.app/?share={base64.b64encode(entry_id.encode()).decode()}"

# UI (gi·ªØ nguy√™n)
st.sidebar.title("‚öôÔ∏è C√†i ƒê·∫∑t")
lang_name = st.sidebar.selectbox("Ng√¥n Ng·ªØ / Language", options=list(LANGUAGES.keys()), index=list(LANGUAGES.keys()).index('vietnamese') if 'vietnamese' in LANGUAGES else 0)
lang_code = LANGUAGES.get(lang_name.lower(), 'vi')
ui_texts = get_ui_texts(lang_name.lower())

if 'history' not in st.session_state:
    st.session_state.history = []

st.sidebar.subheader(ui_texts['history_title'])
if st.session_state.history:
    for i, entry in enumerate(reversed(st.session_state.history)):
        with st.sidebar.expander(f"Entry {len(st.session_state.history)-i} - {entry['timestamp']}"):
            st.image(entry['annotated_b64'], caption="Annotated Image")
            st.text(entry['result'][:200] + "...")
            col1, col2, col3, col4 = st.columns(4)
            with col1: download_text(entry['result'], f"palm_result_{entry['id']}.txt")
            with col2:
                img_data = base64.b64decode(entry['annotated_b64'].split(',')[1])
                st.download_button("üì• Img", img_data, f"palm_img_{entry['id']}.png", "image/png")
            with col3:
                img_array = cv2.imdecode(np.frombuffer(base64.b64decode(entry['annotated_b64'].split(',')[1]), np.uint8), cv2.IMREAD_COLOR)
                create_pdf(img_array, entry['result'], f"palm_pdf_{entry['id']}.pdf")
            with col4:
                share_link = generate_share_link(entry['id'])
                st.code(share_link)
else:
    st.sidebar.info(ui_texts['no_history'])

st.title(translate_text(ui_texts['title'], lang_code))

uploaded_file = st.file_uploader(translate_text(ui_texts['upload_label'], lang_code), type=['jpg', 'jpeg', 'png'])
if uploaded_file is not None:
    image = Image.open(uploaded_file)
    st.image(image, caption=translate_text(ui_texts['original_caption'], lang_code), use_column_width=True)
    
    image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    annotated, raw_result = process_palm(image_cv)
    
    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
    annotated_pil = Image.fromarray(annotated_rgb)
    translated_result = translate_text(raw_result, lang_code)
    
    st.image(annotated_pil, caption=translate_text(ui_texts['annotated_caption'], lang_code), use_column_width=True)
    st.markdown(translated_result)
    st.markdown(translate_text(ui_texts['note'], lang_code))
    
    # History & share (gi·ªØ nguy√™n)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    entry_id = base64.b64encode(os.urandom(8)).decode()
    _, annotated_b64 = cv2.imencode('.png', annotated_rgb)
    b64_str = "data:image/png;base64," + base64.b64encode(annotated_b64).decode()
    
    st.session_state.history.append({
        'id': entry_id,
        'timestamp': timestamp,
        'result': translated_result,
        'annotated_b64': b64_str
    })
    
    col1, col2, col3 = st.columns(3)
    with col1: download_text(translated_result, f"palm_result_{entry_id}.txt")
    with col2:
        bio = io.BytesIO()
        annotated_pil.save(bio, format='PNG')
        st.download_button("üì• Img", bio.getvalue(), f"palm_img_{entry_id}.png", "image/png")
    with col3: create_pdf(annotated_rgb, translated_result, f"palm_pdf_{entry_id}.pdf")
    
    st.info(f"ƒê√£ l∆∞u v√†o l·ªãch s·ª≠! Link share: {generate_share_link(entry_id)}")

st.markdown("---")
st.info("App open-source. Deploy tr√™n Streamlit Cloud ƒë·ªÉ share d·ªÖ d√†ng!")
